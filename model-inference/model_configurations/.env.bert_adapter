# Corresponds to the Huggingface name for finetuned Transformers or the name of a finetuned SentenceTransformers
MODEL_NAME=bert-base-uncased
# Type of the model, e.g. Transformers, Adapter, ...
# See square_model_inference.core.event_handlers.MODEL_MAPPING for all available names with corresponding model
MODEL_TYPE=adapter

MODEL_CLASS=from_config

# Disable CUDA even if available
DISABLE_GPU=True
# Batch size used for many inputs
BATCH_SIZE=32
# Inputs larger than this size are rejected
MAX_INPUT_SIZE=1024

# Cache directory where model weights are stored
# This is the name for the env variable used by transformers and sentence-transformers package
TRANSFORMERS_CACHE=/etc/huggingface/.cache/

# Flag that decides if returned numpy arrays are returned
# as lists or encoded to base64 (smaller but not easily human readable).
# See the comment in square_model_inference.models.prediction._encode_numpy on information on how to decode
# the base64 string back to the numpy array
RETURN_PLAINTEXT_ARRAYS=False

PRELOADED_ADAPTERS=False
