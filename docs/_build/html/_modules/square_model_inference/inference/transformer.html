<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="width=device-width,initial-scale=1" name="viewport"/>
  <meta content="ie=edge" http-equiv="x-ua-compatible"/>
  <meta content="Copy to clipboard" name="lang:clipboard.copy"/>
  <meta content="Copied to clipboard" name="lang:clipboard.copied"/>
  <meta content="en" name="lang:search.language"/>
  <meta content="True" name="lang:search.pipeline.stopwords"/>
  <meta content="True" name="lang:search.pipeline.trimmer"/>
  <meta content="No matching documents" name="lang:search.result.none"/>
  <meta content="1 matching document" name="lang:search.result.one"/>
  <meta content="# matching documents" name="lang:search.result.other"/>
  <meta content="[\s\-]+" name="lang:search.tokenizer"/>
  <link crossorigin="" href="https://fonts.gstatic.com/" rel="preconnect"/>
  <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&amp;display=fallback" rel="stylesheet"/>
  <style>
   body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
  </style>
  <link href="../../../_static/stylesheets/application.css" rel="stylesheet"/>
  <link href="../../../_static/stylesheets/application-palette.css" rel="stylesheet"/>
  <link href="../../../_static/stylesheets/application-fixes.css" rel="stylesheet"/>
  <link href="../../../_static/fonts/material-icons.css" rel="stylesheet"/>
  <meta content="#3f51b5" name="theme-color"/>
  <script src="../../../_static/javascripts/modernizr.js">
  </script>
  <title>
   square_model_inference.inference.transformer — SQuARE  documentation
  </title>
  <link href="../../../_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="../../../_static/material.css" rel="stylesheet" type="text/css"/>
  <link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css"/>
  <link href="../../../_static/custom.css" rel="stylesheet" type="text/css"/>
  <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js">
  </script>
  <script src="../../../_static/jquery.js">
  </script>
  <script src="../../../_static/underscore.js">
  </script>
  <script src="../../../_static/doctools.js">
  </script>
  <script src="../../../_static/clipboard.min.js">
  </script>
  <script src="../../../_static/copybutton.js">
  </script>
  <link href="../../../genindex.html" rel="index" title="Index"/>
  <link href="../../../search.html" rel="search" title="Search"/>
 </head>
 <body data-md-color-accent="blue" data-md-color-primary="blue-grey" dir="ltr">
  <svg class="md-svg">
   <defs data-children-count="0">
    <svg height="448" id="__github" viewbox="0 0 416 448" width="416" xmlns="http://www.w3.org/2000/svg">
     <path d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z" fill="currentColor">
     </path>
    </svg>
   </defs>
  </svg>
  <input class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
  <input class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
  <label class="md-overlay" data-md-component="overlay" for="__drawer">
  </label>
  <a class="md-skip" href="#_modules/square_model_inference/inference/transformer" tabindex="1">
   Skip to content
  </a>
  <header class="md-header" data-md-component="header">
   <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
     <div class="md-flex__cell md-flex__cell--shrink">
      <a class="md-header-nav__button md-logo" href="../../../index.html" title="SQuARE  documentation">
       <img alt="SQuARE documentation logo" height="26" src="../../../_static/SQ_Web_Light_90px.png"/>
      </a>
     </div>
     <div class="md-flex__cell md-flex__cell--shrink">
      <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer">
      </label>
     </div>
     <div class="md-flex__cell md-flex__cell--stretch">
      <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
       <span class="md-header-nav__topic">
        SQuARE
       </span>
       <span class="md-header-nav__topic">
        square_model_inference.inference.transformer
       </span>
      </div>
     </div>
     <div class="md-flex__cell md-flex__cell--shrink">
      <label class="md-icon md-icon--search md-header-nav__button" for="__search">
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
       <label class="md-search__overlay" for="__search">
       </label>
       <div class="md-search__inner" role="search">
        <form action="../../../search.html" class="md-search__form" method="get" name="search">
         <input autocapitalize="off" autocomplete="off" class="md-search__input" data-md-component="query" data-md-state="active" name="q" placeholder="Search" spellcheck="false" type="text"/>
         <label class="md-icon md-search__icon" for="__search">
         </label>
         <button class="md-icon md-search__icon" data-md-component="reset" tabindex="-1" type="reset">
          
         </button>
        </form>
        <div class="md-search__output">
         <div class="md-search__scrollwrap" data-md-scrollfix="">
          <div class="md-search-result" data-md-component="result">
           <div class="md-search-result__meta">
            Type to start searching
           </div>
           <ol class="md-search-result__list">
           </ol>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="md-flex__cell md-flex__cell--shrink">
      <div class="md-header-nav__source">
       <a class="md-source" data-md-source="github" href="https://github.com/UKP-SQuARE/square-core" title="Go to repository">
        <div class="md-source__icon">
         <svg height="28" viewbox="0 0 24 24" width="28" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
          <use height="24" width="24" xlink:href="#__github">
          </use>
         </svg>
        </div>
        <div class="md-source__repository">
         square-core
        </div>
       </a>
      </div>
     </div>
     <div class="md-flex__cell md-flex__cell--shrink dropdown">
      <button class="dropdownbutton">
       Versions
      </button>
      <div class="dropdown-content md-hero">
       <a href="" title="v0.0.1">
        v0.0.1
       </a>
      </div>
     </div>
    </div>
   </nav>
  </header>
  <div class="md-container">
   <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
     <ul class="md-tabs__list">
      <li class="md-tabs__item">
       <a class="md-tabs__link" href="../../index.html">
        Module code
       </a>
      </li>
     </ul>
    </div>
   </nav>
   <main class="md-main">
    <div class="md-main__inner md-grid" data-md-component="container">
     <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
      <div class="md-sidebar__scrollwrap">
       <div class="md-sidebar__inner">
        <nav class="md-nav md-nav--primary" data-md-level="0">
         <label class="md-nav__title md-nav__title--site" for="__drawer">
          <a class="md-nav__button md-logo" href="../../../index.html" title="SQuARE documentation">
           <img alt=" logo" height="48" src="../../../_static/SQ_Web_Light_90px.png" width="48"/>
          </a>
          <a href="../../../index.html" title="SQuARE documentation">
           SQuARE
          </a>
         </label>
         <div class="md-nav__source">
          <a class="md-source" data-md-source="github" href="https://github.com/UKP-SQuARE/square-core" title="Go to repository">
           <div class="md-source__icon">
            <svg height="28" viewbox="0 0 24 24" width="28" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
             <use height="24" width="24" xlink:href="#__github">
             </use>
            </svg>
           </div>
           <div class="md-source__repository">
            square-core
           </div>
          </a>
         </div>
         <ul class="md-nav__list">
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             Overview
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/overview/get_started.html">
            Get Started
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/overview/use_cases.html">
            Use Cases
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/overview/tutorials.html">
            Tutorials
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/overview/roadmap.html">
            Roadmap
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/overview/faq.html">
            FAQ
           </a>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             Components
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/components/datastores.html">
            Datastores
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/components/models.html">
            Models
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/components/skills.html">
            Skills
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/components/explainability.html">
            Explainability
           </a>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             API
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/api/datastore_api/datastores.html">
            Datastore API
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/api/model_api/models.html">
            Model API
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/api/skill_api/skills.html">
            Skill API
           </a>
          </li>
         </ul>
        </nav>
       </div>
      </div>
     </div>
     <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
      <div class="md-sidebar__scrollwrap">
       <div class="md-sidebar__inner">
        <nav class="md-nav md-nav--secondary">
         <ul class="md-nav__list" data-md-scrollfix="">
          <li class="md-nav__item" id="searchbox">
          </li>
         </ul>
        </nav>
       </div>
      </div>
     </div>
     <div class="md-content">
      <article class="md-content__inner md-typeset" role="main">
       <h1 id="modules-square-model-inference-inference-transformer--page-root">
        Source code for square_model_inference.inference.transformer
       </h1>
       <div class="highlight">
        <pre>
<span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> \
    <span class="n">AutoModelForTokenClassification</span><span class="p">,</span> <span class="n">AutoModelForQuestionAnswering</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="kn">from</span> <span class="nn">square_model_inference.inference.model</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">square_model_inference.models.request</span> <span class="kn">import</span> <span class="n">PredictionRequest</span><span class="p">,</span> <span class="n">Task</span>

<span class="kn">from</span> <span class="nn">square_model_inference.models.prediction</span> <span class="kn">import</span> <span class="n">PredictionOutput</span><span class="p">,</span> <span class="n">PredictionOutputForSequenceClassification</span><span class="p">,</span> <span class="n">PredictionOutputForTokenClassification</span><span class="p">,</span> \
    <span class="n">PredictionOutputForQuestionAnswering</span><span class="p">,</span> <span class="n">PredictionOutputForGeneration</span><span class="p">,</span> <span class="n">PredictionOutputForEmbedding</span>

<span class="kn">import</span> <span class="nn">logging</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">CLASS_MAPPING</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"base"</span><span class="p">:</span> <span class="n">AutoModel</span><span class="p">,</span>
    <span class="s2">"sequence_classification"</span><span class="p">:</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span>
    <span class="s2">"token_classification"</span><span class="p">:</span> <span class="n">AutoModelForTokenClassification</span><span class="p">,</span>
    <span class="s2">"question_answering"</span><span class="p">:</span> <span class="n">AutoModelForQuestionAnswering</span><span class="p">,</span>
    <span class="s2">"generation"</span><span class="p">:</span> <span class="n">AutoModelForCausalLM</span>
<span class="p">}</span>

<div class="viewcode-block" id="Transformer"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.transformer.Transformer">[docs]</a><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">Model</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    The class for all Huggingface transformer-based models</span>
<span class="sd">    """</span>
    <span class="n">SUPPORTED_EMBEDDING_MODES</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"mean"</span><span class="p">,</span> <span class="s2">"max"</span><span class="p">,</span> <span class="s2">"cls"</span><span class="p">,</span> <span class="s2">"token"</span><span class="p">,</span> <span class="s2">"pooler"</span><span class="p">]</span>

<div class="viewcode-block" id="Transformer.__init__"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.transformer.Transformer.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">model_class</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">disable_gpu</span><span class="p">,</span> <span class="n">max_input_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Initialize the Transformer</span>

<span class="sd">        Args:</span>
<span class="sd">             model_name: the Huggingface model name</span>
<span class="sd">             model_class: the class name (according to CLASS_MAPPING) to use</span>
<span class="sd">             batch_size: batch size used for inference</span>
<span class="sd">             disable_gpu: do not move model to GPU even if CUDA is available</span>
<span class="sd">             max_input_size: requests with a larger input are rejected</span>
<span class="sd">             kwargs: Not used</span>
<span class="sd">        """</span>
        <span class="k">if</span> <span class="n">model_class</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">CLASS_MAPPING</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Unknown MODEL_CLASS. Must be one of </span><span class="si">{</span><span class="n">CLASS_MAPPING</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_load_model</span><span class="p">(</span><span class="n">CLASS_MAPPING</span><span class="p">[</span><span class="n">model_class</span><span class="p">],</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">disable_gpu</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_input_size</span> <span class="o">=</span> <span class="n">max_input_size</span></div>

<div class="viewcode-block" id="Transformer._load_model"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.transformer.Transformer._load_model">[docs]</a>    <span class="k">def</span> <span class="nf">_load_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_cls</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">disable_gpu</span><span class="p">):</span>

        <span class="sd">"""</span>
<span class="sd">        Load the Transformer model model_name and its tokenizer with Huggingface.</span>
<span class="sd">        Model will be moved to GPU unless CUDA is unavailable or disable_gpu is true.</span>
<span class="sd">        """</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Loading model </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
        <span class="c1"># Check if GPU is available</span>
        <span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">disable_gpu</span> <span class="k">else</span> <span class="s2">"cpu"</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">model_cls</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> loaded on </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span></div>

<div class="viewcode-block" id="Transformer._ensure_tensor_on_device"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.transformer.Transformer._ensure_tensor_on_device">[docs]</a>    <span class="k">def</span> <span class="nf">_ensure_tensor_on_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">inputs</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Ensure PyTorch tensors are on the specified device.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs (keyword arguments that should be :obj:`torch.Tensor`): The tensors to place on :obj:`self.device`.</span>

<span class="sd">        Return:</span>
<span class="sd">            :obj:`Dict[str, torch.Tensor]`: The same as :obj:`inputs` but on the proper device.</span>
<span class="sd">        """</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span></div>

<div class="viewcode-block" id="Transformer._predict"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.transformer.Transformer._predict">[docs]</a>    <span class="k">def</span> <span class="nf">_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">:</span> <span class="n">PredictionRequest</span><span class="p">,</span> <span class="n">output_features</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> \
            <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]]:</span>
        <span class="sd">"""</span>
<span class="sd">        Inference on the input.</span>

<span class="sd">        Args:</span>
<span class="sd">         request: the request with the input and optional kwargs</span>
<span class="sd">         output_features: return the features of the input.</span>
<span class="sd">            Necessary if, e.g., attention mask is needed for post-processing.</span>

<span class="sd">        Returns:</span>
<span class="sd">             The model outputs and optionally the input features</span>
<span class="sd">        """</span>
        <span class="n">all_predictions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="p">[</span><span class="s2">"padding"</span><span class="p">]</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"padding"</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="p">[</span><span class="s2">"truncation"</span><span class="p">]</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"truncation"</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">input</span><span class="p">,</span>
                                  <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span>
                                  <span class="o">**</span><span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">start_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">input</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">input_features</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">features</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">start_idx</span><span class="p">:</span><span class="n">start_idx</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">features</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>
                <span class="n">input_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_tensor_on_device</span><span class="p">(</span><span class="o">**</span><span class="n">input_features</span><span class="p">)</span>
                <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_features</span><span class="p">,</span> <span class="o">**</span><span class="n">request</span><span class="o">.</span><span class="n">model_kwargs</span><span class="p">)</span>
                <span class="n">all_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">all_predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="n">final_prediction</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">:</span>
            <span class="c1"># HuggingFace outputs for 'attentions' and more is returned as tuple of tensors</span>
            <span class="c1"># Tuple of tuples only exists for 'past_key_values' which is only relevant for generation.</span>
            <span class="c1"># Generation should NOT use this function</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">all_predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">key</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="n">tuple_of_lists</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="k">else</span> <span class="n">p</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">tpl</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span> <span class="k">for</span> <span class="n">tpl</span> <span class="ow">in</span> <span class="n">all_predictions</span><span class="p">]))</span>
                <span class="n">final_prediction</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">tuple_of_lists</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">final_prediction</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">p</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">all_predictions</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">output_features</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">final_prediction</span><span class="p">,</span> <span class="n">features</span>
        <span class="k">return</span> <span class="n">final_prediction</span></div>

    <span class="k">def</span> <span class="nf">_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">:</span> <span class="n">PredictionRequest</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PredictionOutput</span><span class="p">:</span>
        <span class="n">request</span><span class="o">.</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">"output_hidden_states"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">predictions</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="n">output_features</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># We remove hidden_states from predictions!0</span>
        <span class="k">if</span> <span class="s2">"hidden_states"</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">:</span>
            <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"hidden_states"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">elif</span> <span class="s2">"last_hidden_state"</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">:</span>
            <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"last_hidden_state"</span><span class="p">)</span>
        <span class="k">elif</span> <span class="s2">"decoder_hidden_states"</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">:</span>
            <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"decoder_hidden_states"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"No hidden state available in keys: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predictions</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="s2">"attention_mask"</span><span class="p">]</span>

        <span class="n">embedding_mode</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"embedding_mode"</span><span class="p">,</span> <span class="s2">"mean"</span><span class="p">)</span>
        <span class="n">task_outputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"embedding_mode"</span><span class="p">:</span> <span class="n">embedding_mode</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="n">embedding_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">SUPPORTED_EMBEDDING_MODES</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Embedding mode </span><span class="si">{</span><span class="n">embedding_mode</span><span class="si">}</span><span class="s2"> not in list of supported modes </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">SUPPORTED_EMBEDDING_MODES</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">embedding_mode</span> <span class="o">==</span> <span class="s2">"cls"</span><span class="p">:</span>
            <span class="n">emb</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">elif</span> <span class="n">embedding_mode</span> <span class="o">==</span> <span class="s2">"pooler"</span><span class="p">:</span>
            <span class="n">emb</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="s2">"pooler_output"</span><span class="p">]</span>
        <span class="c1"># copied from sentence-transformers pooling</span>
        <span class="k">elif</span> <span class="n">embedding_mode</span> <span class="o">==</span> <span class="s2">"max"</span><span class="p">:</span>
            <span class="n">input_mask_expanded</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">hidden_state</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">hidden_state</span><span class="p">[</span><span class="n">input_mask_expanded</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1e9</span>  <span class="c1"># Set padding tokens to large negative value</span>
            <span class="n">emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># copied from sentence-transformers pooling</span>
        <span class="k">elif</span> <span class="n">embedding_mode</span> <span class="o">==</span> <span class="s2">"mean"</span><span class="p">:</span>
            <span class="n">input_mask_expanded</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">hidden_state</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">sum_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">hidden_state</span> <span class="o">*</span> <span class="n">input_mask_expanded</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">sum_mask</span> <span class="o">=</span> <span class="n">input_mask_expanded</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">emb</span> <span class="o">=</span> <span class="n">sum_embeddings</span> <span class="o">/</span> <span class="n">sum_mask</span>
        <span class="k">elif</span> <span class="n">embedding_mode</span> <span class="o">==</span> <span class="s2">"token"</span><span class="p">:</span>
            <span class="n">emb</span> <span class="o">=</span> <span class="n">hidden_state</span>
            <span class="n">task_outputs</span><span class="p">[</span><span class="s2">"word_ids"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">features</span><span class="o">.</span><span class="n">word_ids</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">input</span><span class="p">))]</span>
        <span class="n">predictions</span><span class="p">[</span><span class="s2">"embeddings"</span><span class="p">]</span> <span class="o">=</span> <span class="n">emb</span>

        <span class="k">return</span> <span class="n">PredictionOutputForEmbedding</span><span class="p">(</span><span class="n">model_outputs</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="o">**</span><span class="n">task_outputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_token_classification</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">:</span> <span class="n">PredictionRequest</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PredictionOutput</span><span class="p">:</span>
        <span class="n">predictions</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="n">output_features</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># If logits dim &gt; 1 or if the 'is_regression' flag is not set, we assume classification:</span>
        <span class="c1"># We replace the logits by the softmax and add labels chosen with argmax</span>
        <span class="n">label2id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">label2id</span>
        <span class="n">id2label</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">label2id</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="n">task_outputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"id2label"</span><span class="p">:</span> <span class="n">id2label</span><span class="p">,</span>
            <span class="s2">"word_ids"</span><span class="p">:</span> <span class="p">[</span><span class="n">features</span><span class="o">.</span><span class="n">word_ids</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">input</span><span class="p">))]</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"is_regression"</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">]</span> <span class="o">=</span> <span class="n">probabilities</span>
            <span class="n">task_outputs</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">PredictionOutputForTokenClassification</span><span class="p">(</span><span class="n">model_outputs</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="o">**</span><span class="n">task_outputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sequence_classification</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">:</span> <span class="n">PredictionRequest</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PredictionOutput</span><span class="p">:</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
        <span class="n">label2id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">label2id</span>
        <span class="n">id2label</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">label2id</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="n">task_outputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"id2label"</span><span class="p">:</span> <span class="n">id2label</span>
        <span class="p">}</span>
        <span class="c1"># If logits dim &gt; 1 or if the 'is_regression' flag is not set, we assume classification:</span>
        <span class="c1"># We replace the logits by the softmax and add labels chosen with argmax</span>
        <span class="k">if</span> <span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"is_regression"</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">]</span> <span class="o">=</span> <span class="n">probabilities</span>
            <span class="n">task_outputs</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">PredictionOutputForSequenceClassification</span><span class="p">(</span><span class="n">model_outputs</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="o">**</span><span class="n">task_outputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">:</span> <span class="n">PredictionRequest</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PredictionOutput</span><span class="p">:</span>
        <span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="p">[</span><span class="s2">"padding"</span><span class="p">]</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"padding"</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="p">[</span><span class="s2">"add_special_tokens"</span><span class="p">]</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"add_special_tokens"</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">task_outputs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"generated_texts"</span><span class="p">:</span> <span class="p">[]}</span>
        <span class="n">model_outputs</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="c1"># We cannot batch generate so we have to to it separately for each input prompt.</span>
        <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">request</span><span class="o">.</span><span class="n">input</span><span class="p">:</span>
            <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="o">**</span><span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="p">)</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_tensor_on_device</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">)[</span><span class="s2">"input_ids"</span><span class="p">]</span>
            <span class="n">request</span><span class="o">.</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="p">)</span>
            <span class="n">request</span><span class="o">.</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">"return_dict_in_generate"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">request</span><span class="o">.</span><span class="n">model_kwargs</span><span class="p">)</span>

            <span class="c1"># put everything on CPU and add it to model_outputs</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">):</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">):</span>
                        <span class="n">res</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">((</span><span class="nb">tuple</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tpl</span><span class="p">))</span> <span class="k">for</span> <span class="n">tpl</span> <span class="ow">in</span> <span class="n">res</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">res</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">res</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">res</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
                <span class="n">model_outputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>

            <span class="n">generated_texts</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                         <span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"clean_up_tokenization_spaces"</span><span class="p">,</span> <span class="kc">False</span><span class="p">))</span>
                               <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">res</span><span class="p">[</span><span class="s2">"sequences"</span><span class="p">]]</span>
            <span class="n">task_outputs</span><span class="p">[</span><span class="s2">"generated_texts"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">generated_texts</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">PredictionOutputForGeneration</span><span class="p">(</span><span class="n">model_outputs</span><span class="o">=</span><span class="n">model_outputs</span><span class="p">,</span> <span class="o">**</span><span class="n">task_outputs</span><span class="p">)</span>

<div class="viewcode-block" id="Transformer._question_answering"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.transformer.Transformer._question_answering">[docs]</a>    <span class="k">def</span> <span class="nf">_question_answering</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">:</span> <span class="n">PredictionRequest</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PredictionOutput</span><span class="p">:</span>
        <span class="sd">"""</span>
<span class="sd">        Span-based question answering for a given question and context.</span>
<span class="sd">        We expect the input to use the (question, context) format for the text pairs.</span>

<span class="sd">        Args:</span>
<span class="sd">          request: the prediction request</span>

<span class="sd">        """</span>
        <span class="c1"># Making heavy use of https://huggingface.co/transformers/_modules/transformers/pipelines/question_answering.html#QuestionAnsweringPipeline</span>
        <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">start_</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">end_</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">topk</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_answer_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">undesired_tokens_</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">:</span>
                <span class="sd">"""</span>
<span class="sd">                Take the output of any :obj:`ModelForQuestionAnswering` and will generate probabilities for each span to be the</span>
<span class="sd">                actual answer.</span>

<span class="sd">                In addition, it filters out some unwanted/impossible cases like answer len being greater than max_answer_len or</span>
<span class="sd">                answer end position being before the starting position. The method supports output the k-best answer through</span>
<span class="sd">                the topk argument.</span>

<span class="sd">                Args:</span>
<span class="sd">                    start_ (:obj:`np.ndarray`): Individual start probabilities for each token.</span>
<span class="sd">                    end (:obj:`np.ndarray`): Individual end_ probabilities for each token.</span>
<span class="sd">                    topk (:obj:`int`): Indicates how many possible answer span(s) to extract from the model output.</span>
<span class="sd">                    max_answer_len (:obj:`int`): Maximum size of the answer to extract from the model's output.</span>
<span class="sd">                    undesired_tokens_ (:obj:`np.ndarray`): Mask determining tokens that can be part of the answer</span>
<span class="sd">                """</span>
                <span class="c1"># Ensure we have batch axis</span>
                <span class="k">if</span> <span class="n">start_</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">start_</span> <span class="o">=</span> <span class="n">start_</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">end_</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">end_</span> <span class="o">=</span> <span class="n">end_</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span>

                <span class="c1"># Compute the score of each tuple(start_, end_) to be the real answer</span>
                <span class="n">outer</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">start_</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">end_</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

                <span class="c1"># Remove candidate with end_ &lt; start_ and end_ - start_ &gt; max_answer_len</span>
                <span class="n">candidates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">outer</span><span class="p">),</span> <span class="n">max_answer_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

                <span class="c1">#  Inspired by Chen &amp; al. (https://github.com/facebookresearch/DrQA)</span>
                <span class="n">scores_flat</span> <span class="o">=</span> <span class="n">candidates</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">topk</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">idx_sort</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores_flat</span><span class="p">)]</span>
                <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">scores_flat</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">topk</span><span class="p">:</span>
                    <span class="n">idx_sort</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="n">scores_flat</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argpartition</span><span class="p">(</span><span class="o">-</span><span class="n">scores_flat</span><span class="p">,</span> <span class="n">topk</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="n">topk</span><span class="p">]</span>
                    <span class="n">idx_sort</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="n">scores_flat</span><span class="p">[</span><span class="n">idx</span><span class="p">])]</span>

                <span class="n">starts_</span><span class="p">,</span> <span class="n">ends_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unravel_index</span><span class="p">(</span><span class="n">idx_sort</span><span class="p">,</span> <span class="n">candidates</span><span class="o">.</span><span class="n">shape</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span>
                <span class="n">desired_spans</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">starts_</span><span class="p">,</span> <span class="n">undesired_tokens_</span><span class="o">.</span><span class="n">nonzero</span><span class="p">())</span> <span class="o">&amp;</span> <span class="n">np</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">ends_</span><span class="p">,</span> <span class="n">undesired_tokens_</span><span class="o">.</span><span class="n">nonzero</span><span class="p">())</span>
                <span class="n">starts_</span> <span class="o">=</span> <span class="n">starts_</span><span class="p">[</span><span class="n">desired_spans</span><span class="p">]</span>
                <span class="n">ends_</span> <span class="o">=</span> <span class="n">ends_</span><span class="p">[</span><span class="n">desired_spans</span><span class="p">]</span>
                <span class="n">scores_</span> <span class="o">=</span> <span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">starts_</span><span class="p">,</span> <span class="n">ends_</span><span class="p">]</span>

                <span class="k">return</span> <span class="n">starts_</span><span class="p">,</span> <span class="n">ends_</span><span class="p">,</span> <span class="n">scores_</span>

        <span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="p">[</span><span class="s2">"truncation"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"only_second"</span>
        <span class="n">predictions</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="n">output_features</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">task_outputs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"answers"</span><span class="p">:</span> <span class="p">[]}</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">context</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s2">"start_logits"</span><span class="p">],</span> <span class="n">predictions</span><span class="p">[</span><span class="s2">"end_logits"</span><span class="p">],</span> <span class="n">request</span><span class="o">.</span><span class="n">input</span><span class="p">)):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">start</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">end</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="c1"># Ensure padded tokens &amp; question tokens cannot belong to the set of candidate answers.</span>
            <span class="n">question_tokens</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">s</span> <span class="o">!=</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">features</span><span class="o">.</span><span class="n">sequence_ids</span><span class="p">(</span><span class="n">idx</span><span class="p">)])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="c1"># Unmask CLS token for 'no answer'</span>
            <span class="n">question_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">undesired_tokens</span> <span class="o">=</span> <span class="n">question_tokens</span> <span class="o">&amp;</span> <span class="n">features</span><span class="p">[</span><span class="s2">"attention_mask"</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

            <span class="c1"># Generate mask</span>
            <span class="n">undesired_tokens_mask</span> <span class="o">=</span> <span class="n">undesired_tokens</span> <span class="o">==</span> <span class="mf">0.0</span>

            <span class="c1"># Make sure non-context indexes in the tensor cannot contribute to the softmax</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">undesired_tokens_mask</span><span class="p">,</span> <span class="o">-</span><span class="mf">10000.0</span><span class="p">,</span> <span class="n">start</span><span class="p">)</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">undesired_tokens_mask</span><span class="p">,</span> <span class="o">-</span><span class="mf">10000.0</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span>

            <span class="n">start</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">start</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">start</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">end</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>

            <span class="c1"># Get score for 'no answer' then mask for decoding step (CLS token</span>
            <span class="n">no_answer_score</span> <span class="o">=</span> <span class="p">(</span><span class="n">start</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">end</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">start</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">end</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

            <span class="n">starts</span><span class="p">,</span> <span class="n">ends</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">decode</span><span class="p">(</span>
                <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"topk"</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"max_answer_len"</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">undesired_tokens</span>
            <span class="p">)</span>
            <span class="n">enc</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="n">answers</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">"score"</span><span class="p">:</span> <span class="n">score</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                    <span class="s2">"start"</span><span class="p">:</span> <span class="n">enc</span><span class="o">.</span><span class="n">word_to_chars</span><span class="p">(</span>
                        <span class="n">enc</span><span class="o">.</span><span class="n">token_to_word</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">sequence_index</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>
                    <span class="s2">"end"</span><span class="p">:</span> <span class="n">enc</span><span class="o">.</span><span class="n">word_to_chars</span><span class="p">(</span><span class="n">enc</span><span class="o">.</span><span class="n">token_to_word</span><span class="p">(</span><span class="n">e</span><span class="p">),</span> <span class="n">sequence_index</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span>
                    <span class="s2">"answer"</span><span class="p">:</span> <span class="n">context</span><span class="p">[</span>
                              <span class="n">enc</span><span class="o">.</span><span class="n">word_to_chars</span><span class="p">(</span><span class="n">enc</span><span class="o">.</span><span class="n">token_to_word</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">sequence_index</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="p">:</span>
                              <span class="n">enc</span><span class="o">.</span><span class="n">word_to_chars</span><span class="p">(</span><span class="n">enc</span><span class="o">.</span><span class="n">token_to_word</span><span class="p">(</span><span class="n">e</span><span class="p">),</span> <span class="n">sequence_index</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]],</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">starts</span><span class="p">,</span> <span class="n">ends</span><span class="p">,</span> <span class="n">scores</span><span class="p">)]</span>
            <span class="n">answers</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">"score"</span><span class="p">:</span> <span class="n">no_answer_score</span><span class="p">,</span> <span class="s2">"start"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">"end"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">"answer"</span><span class="p">:</span> <span class="s2">""</span><span class="p">})</span>
            <span class="n">answers</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">answers</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">"score"</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span> <span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"topk"</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
            <span class="n">task_outputs</span><span class="p">[</span><span class="s2">"answers"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">answers</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">PredictionOutputForQuestionAnswering</span><span class="p">(</span><span class="n">model_outputs</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="o">**</span><span class="n">task_outputs</span><span class="p">)</span></div>

<div class="viewcode-block" id="Transformer.predict"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.transformer.Transformer.predict">[docs]</a>    <span class="k">async</span> <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">:</span> <span class="n">PredictionRequest</span><span class="p">,</span> <span class="n">task</span><span class="p">:</span> <span class="n">Task</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PredictionOutput</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">request</span><span class="o">.</span><span class="n">is_preprocessed</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"is_preprocessed=True is not supported for this model. Please use text as input."</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">input</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_input_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Input is too large. Max input size is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_input_size</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">task</span> <span class="o">==</span> <span class="n">Task</span><span class="o">.</span><span class="n">sequence_classification</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sequence_classification</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">task</span> <span class="o">==</span> <span class="n">Task</span><span class="o">.</span><span class="n">token_classification</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_token_classification</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">task</span> <span class="o">==</span> <span class="n">Task</span><span class="o">.</span><span class="n">embedding</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embedding</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">task</span> <span class="o">==</span> <span class="n">Task</span><span class="o">.</span><span class="n">question_answering</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_question_answering</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">task</span> <span class="o">==</span> <span class="n">Task</span><span class="o">.</span><span class="n">generation</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generation</span><span class="p">(</span><span class="n">request</span><span class="p">)</span></div></div>

</pre>
       </div>
      </article>
     </div>
    </div>
   </main>
  </div>
  <footer class="footer-basic">
   <!--            <div class="social"><a href="#"><i class="icon ion-social-instagram"></i></a><a href="#"><i class="icon ion-social-snapchat"></i></a><a href="#"><i class="icon ion-social-twitter"></i></a><a href="#"><i class="icon ion-social-facebook"></i></a></div>-->
   <div class="footer-links">
    <a class="footer-refs" href="https://www.informatik.tu-darmstadt.de/ukp/impressum.en.jsp">
     Legal Note
    </a>
    <a class="footer-refs" href="https://www.tu-darmstadt.de/datenschutzerklaerung.en.jsp">
     Privacy Policy
    </a>
    <a class="footer-refs" href="https://www.informatik.tu-darmstadt.de/ukp/ukp_home/index.en.jsp">
     UKP Lab
    </a>
   </div>
   <p class="copyright">
    UKP © 2022
   </p>
  </footer>
 </body>
</html>