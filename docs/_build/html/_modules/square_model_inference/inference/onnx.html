<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="width=device-width,initial-scale=1" name="viewport"/>
  <meta content="ie=edge" http-equiv="x-ua-compatible"/>
  <meta content="Copy to clipboard" name="lang:clipboard.copy"/>
  <meta content="Copied to clipboard" name="lang:clipboard.copied"/>
  <meta content="en" name="lang:search.language"/>
  <meta content="True" name="lang:search.pipeline.stopwords"/>
  <meta content="True" name="lang:search.pipeline.trimmer"/>
  <meta content="No matching documents" name="lang:search.result.none"/>
  <meta content="1 matching document" name="lang:search.result.one"/>
  <meta content="# matching documents" name="lang:search.result.other"/>
  <meta content="[\s\-]+" name="lang:search.tokenizer"/>
  <link crossorigin="" href="https://fonts.gstatic.com/" rel="preconnect"/>
  <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&amp;display=fallback" rel="stylesheet"/>
  <style>
   body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
  </style>
  <link href="../../../_static/stylesheets/application.css" rel="stylesheet"/>
  <link href="../../../_static/stylesheets/application-palette.css" rel="stylesheet"/>
  <link href="../../../_static/stylesheets/application-fixes.css" rel="stylesheet"/>
  <link href="../../../_static/fonts/material-icons.css" rel="stylesheet"/>
  <meta content="#3f51b5" name="theme-color"/>
  <script src="../../../_static/javascripts/modernizr.js">
  </script>
  <title>
   square_model_inference.inference.onnx — SQuARE  documentation
  </title>
  <link href="../../../_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="../../../_static/material.css" rel="stylesheet" type="text/css"/>
  <link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css"/>
  <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js">
  </script>
  <script src="../../../_static/jquery.js">
  </script>
  <script src="../../../_static/underscore.js">
  </script>
  <script src="../../../_static/doctools.js">
  </script>
  <script src="../../../_static/clipboard.min.js">
  </script>
  <script src="../../../_static/copybutton.js">
  </script>
  <link href="../../../genindex.html" rel="index" title="Index"/>
  <link href="../../../search.html" rel="search" title="Search"/>
 </head>
 <body data-md-color-accent="#086494" data-md-color-primary="#086494" dir="ltr">
  <svg class="md-svg">
   <defs data-children-count="0">
    <svg height="448" id="__github" viewbox="0 0 416 448" width="416" xmlns="http://www.w3.org/2000/svg">
     <path d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z" fill="currentColor">
     </path>
    </svg>
   </defs>
  </svg>
  <input class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
  <input class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
  <label class="md-overlay" data-md-component="overlay" for="__drawer">
  </label>
  <a class="md-skip" href="#_modules/square_model_inference/inference/onnx" tabindex="1">
   Skip to content
  </a>
  <header class="md-header" data-md-component="header">
   <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
     <div class="md-flex__cell md-flex__cell--shrink">
      <a class="md-header-nav__button md-logo" href="../../../index.html" title="SQuARE  documentation">
       <img alt="SQuARE documentation logo" height="26" src="../../../_static/SQ_Web_Dark_160px.png"/>
      </a>
     </div>
     <div class="md-flex__cell md-flex__cell--shrink">
      <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer">
      </label>
     </div>
     <div class="md-flex__cell md-flex__cell--stretch">
      <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
       <span class="md-header-nav__topic">
        SQuARE
       </span>
       <span class="md-header-nav__topic">
        square_model_inference.inference.onnx
       </span>
      </div>
     </div>
     <div class="md-flex__cell md-flex__cell--shrink">
      <label class="md-icon md-icon--search md-header-nav__button" for="__search">
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
       <label class="md-search__overlay" for="__search">
       </label>
       <div class="md-search__inner" role="search">
        <form action="../../../search.html" class="md-search__form" method="get" name="search">
         <input autocapitalize="off" autocomplete="off" class="md-search__input" data-md-component="query" data-md-state="active" name="q" placeholder="Search" spellcheck="false" type="text"/>
         <label class="md-icon md-search__icon" for="__search">
         </label>
         <button class="md-icon md-search__icon" data-md-component="reset" tabindex="-1" type="reset">
          
         </button>
        </form>
        <div class="md-search__output">
         <div class="md-search__scrollwrap" data-md-scrollfix="">
          <div class="md-search-result" data-md-component="result">
           <div class="md-search-result__meta">
            Type to start searching
           </div>
           <ol class="md-search-result__list">
           </ol>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="md-flex__cell md-flex__cell--shrink">
      <div class="md-header-nav__source">
       <a class="md-source" data-md-source="github" href="https://github.com/UKP-SQuARE/square-core" title="Go to repository">
        <div class="md-source__icon">
         <svg height="28" viewbox="0 0 24 24" width="28" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
          <use height="24" width="24" xlink:href="#__github">
          </use>
         </svg>
        </div>
        <div class="md-source__repository">
         square-core
        </div>
       </a>
      </div>
     </div>
     <div class="md-flex__cell md-flex__cell--shrink dropdown">
      <button class="dropdownbutton">
       Versions
      </button>
      <div class="dropdown-content md-hero">
       <a href="" title="v0.0.1">
        v0.0.1
       </a>
      </div>
     </div>
    </div>
   </nav>
  </header>
  <div class="md-container">
   <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
     <ul class="md-tabs__list">
      <li class="md-tabs__item">
       <a class="md-tabs__link" href="../../index.html">
        Module code
       </a>
      </li>
     </ul>
    </div>
   </nav>
   <main class="md-main">
    <div class="md-main__inner md-grid" data-md-component="container">
     <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
      <div class="md-sidebar__scrollwrap">
       <div class="md-sidebar__inner">
        <nav class="md-nav md-nav--primary" data-md-level="0">
         <label class="md-nav__title md-nav__title--site" for="__drawer">
          <a class="md-nav__button md-logo" href="../../../index.html" title="SQuARE documentation">
           <img alt=" logo" height="48" src="../../../_static/SQ_Web_Dark_160px.png" width="48"/>
          </a>
          <a href="../../../index.html" title="SQuARE documentation">
           SQuARE
          </a>
         </label>
         <div class="md-nav__source">
          <a class="md-source" data-md-source="github" href="https://github.com/UKP-SQuARE/square-core" title="Go to repository">
           <div class="md-source__icon">
            <svg height="28" viewbox="0 0 24 24" width="28" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
             <use height="24" width="24" xlink:href="#__github">
             </use>
            </svg>
           </div>
           <div class="md-source__repository">
            square-core
           </div>
          </a>
         </div>
         <ul class="md-nav__list">
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             Overview
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/overview/get_started.html">
            Get Started
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/overview/use_cases.html">
            Use Cases
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/overview/tutorials.html">
            Tutorials
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/overview/roadmap.html">
            Roadmap
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/overview/faq.html">
            FAQ
           </a>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             Components
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/components/datastores.html">
            Datastores
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/components/models.html">
            Models
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/components/skills.html">
            Skills
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/components/explainability.html">
            Explainability
           </a>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             API
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/api/datastore_api/datastores.html">
            Datastore API
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/api/model_api/models.html">
            Model API
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../../../pages/api/skill_api/skills.html">
            Skill API
           </a>
          </li>
         </ul>
        </nav>
       </div>
      </div>
     </div>
     <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
      <div class="md-sidebar__scrollwrap">
       <div class="md-sidebar__inner">
        <nav class="md-nav md-nav--secondary">
         <ul class="md-nav__list" data-md-scrollfix="">
          <li class="md-nav__item" id="searchbox">
          </li>
         </ul>
        </nav>
       </div>
      </div>
     </div>
     <div class="md-content">
      <article class="md-content__inner md-typeset" role="main">
       <h1 id="modules-square-model-inference-inference-onnx--page-root">
        Source code for square_model_inference.inference.onnx
       </h1>
       <div class="highlight">
        <pre>
<span></span><span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>

<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="kn">from</span> <span class="nn">.transformer</span> <span class="kn">import</span> <span class="n">Transformer</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="nn">onnxruntime</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">square_model_inference.models.request</span> <span class="kn">import</span> <span class="n">PredictionRequest</span><span class="p">,</span> <span class="n">Task</span>
<span class="kn">from</span> <span class="nn">square_model_inference.models.prediction</span> <span class="kn">import</span> <span class="n">PredictionOutput</span><span class="p">,</span> <span class="n">PredictionOutputForEmbedding</span><span class="p">,</span> \
    <span class="n">PredictionOutputForSequenceClassification</span><span class="p">,</span> <span class="n">PredictionOutputForGeneration</span><span class="p">,</span> <span class="n">PredictionOutputForTokenClassification</span>


<span class="k">def</span> <span class="nf">to_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">else</span> <span class="n">x</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">x</span>


<div class="viewcode-block" id="Onnx"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.onnx.Onnx">[docs]</a><span class="k">class</span> <span class="nc">Onnx</span><span class="p">(</span><span class="n">Transformer</span><span class="p">):</span>
<div class="viewcode-block" id="Onnx.__init__"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.onnx.Onnx.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">disable_gpu</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                 <span class="n">max_input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">decoder_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

        <span class="sd">"""</span>
<span class="sd">         Args:</span>
<span class="sd">             model_path: patyh where the model is stored</span>
<span class="sd">             model_name: the ONNX model name</span>
<span class="sd">             batch_size: batch size used for inference</span>
<span class="sd">             disable_gpu: do not move model to GPU even if CUDA is available</span>
<span class="sd">             max_input_size: requests with a larger input are rejected</span>
<span class="sd">             decoder_path: path to the decoder ONNX model</span>
<span class="sd">             kwargs: Not used</span>
<span class="sd">        """</span>
        <span class="c1"># This assumes that a corresponding onnx file exists</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
        <span class="c1"># check whether a decoder model is available</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="o">=</span> <span class="n">decoder_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">decoder_path</span> <span class="o">!=</span> <span class="s2">""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="c1"># if available load the decoder model in a onnx session</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder_session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="n">decoder_path</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">diable_gpu</span> <span class="o">=</span> <span class="n">disable_gpu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_input_size</span> <span class="o">=</span> <span class="n">max_input_size</span></div>

<div class="viewcode-block" id="Onnx._predict"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.onnx.Onnx._predict">[docs]</a>    <span class="k">def</span> <span class="nf">_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">:</span> <span class="n">PredictionRequest</span><span class="p">,</span> <span class="n">output_features</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">features</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> \
            <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]]:</span>
        <span class="sd">"""</span>
<span class="sd">        Inference on the input.</span>

<span class="sd">        Args:</span>

<span class="sd">             request: the request with the input and optional kwargs</span>
<span class="sd">             output_features: return the features of the input. Necessary if, e.g., attention mask is needed for post-processing.</span>

<span class="sd">        Returns:</span>
<span class="sd">                The model outputs and optionally the input features</span>
<span class="sd">        """</span>
        <span class="n">all_predictions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="p">[</span><span class="s2">"padding"</span><span class="p">]</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"padding"</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="p">[</span><span class="s2">"truncation"</span><span class="p">]</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"truncation"</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">features</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">input</span><span class="p">,</span>
                                      <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span>
                                      <span class="o">**</span><span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">start_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">features</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">input_names</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()))]</span>
            <span class="n">ort_inputs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
                <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">input_data</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">start_idx</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">]))</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">input_data</span> <span class="ow">in</span> <span class="n">features</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">input_names</span><span class="p">)</span>

            <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">([],</span> <span class="n">ort_inputs</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_session</span><span class="p">:</span>
                    <span class="c1"># Prepare decoder input</span>
                    <span class="c1"># This works with encoder decoder models exported similarirly to the FastT5 onnx model</span>
                    <span class="n">ort_inputs</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="s2">"input_ids"</span><span class="p">:</span> <span class="n">features</span><span class="p">[</span><span class="s2">"decoder_input_ids"</span><span class="p">]</span> <span class="k">if</span> <span class="s2">"decoder_input_ids"</span> <span class="ow">in</span> <span class="n">features</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
                            <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">get_bos_token</span><span class="p">()]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])],</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="s2">"encoder_hidden_states"</span><span class="p">:</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                        <span class="s2">"encoder_attention_mask"</span><span class="p">:</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s2">"attention_mask"</span><span class="p">])}</span>
                    <span class="n">res</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_session</span><span class="o">.</span><span class="n">run</span><span class="p">([],</span> <span class="n">ort_inputs</span><span class="p">)</span>
            <span class="n">all_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
        <span class="n">final_prediction</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">get_outputs</span><span class="p">()[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">get_outputs</span><span class="p">()))]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="n">output_names</span> <span class="o">+=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_session</span><span class="o">.</span><span class="n">get_outputs</span><span class="p">()[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span>
                             <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_session</span><span class="o">.</span><span class="n">get_outputs</span><span class="p">()))]</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output_names</span><span class="p">):</span>
            <span class="c1"># HuggingFace outputs for 'attentions' and more is returned as tuple of tensors</span>
            <span class="c1"># Tuple of tuples only exists for 'past_key_values' which is only relevant for generation.</span>
            <span class="c1"># Generation should NOT use this function</span>
            <span class="n">final_prediction</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">all_predictions</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">output_features</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">final_prediction</span><span class="p">,</span> <span class="n">features</span>
        <span class="k">return</span> <span class="n">final_prediction</span></div>

<div class="viewcode-block" id="Onnx._embedding"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.onnx.Onnx._embedding">[docs]</a>    <span class="k">def</span> <span class="nf">_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">:</span> <span class="n">PredictionRequest</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PredictionOutput</span><span class="p">:</span>
        <span class="sd">"""</span>
<span class="sd">        Embeds the input from the request</span>

<span class="sd">        Args:</span>
<span class="sd">             request: The request containing input and optionally task kwargs like embedding method</span>

<span class="sd">        Return:</span>
<span class="sd">             The embedding output</span>
<span class="sd">        """</span>
        <span class="n">embedding_mode</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"embedding_mode"</span><span class="p">,</span> <span class="s2">"mean"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">embedding_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">SUPPORTED_EMBEDDING_MODES</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"Embedding mode </span><span class="si">{</span><span class="n">embedding_mode</span><span class="si">}</span><span class="s2"> not in list of supported modes </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">SUPPORTED_EMBEDDING_MODES</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

        <span class="n">task_outputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"embedding_mode"</span><span class="p">:</span> <span class="n">embedding_mode</span>
        <span class="p">}</span>
        <span class="n">predictions</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="n">output_features</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">embedding_mode</span> <span class="o">==</span> <span class="s2">"pooler"</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">"pooler_output"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"No pooler output available. Use a different model or an other embedding method"</span><span class="p">)</span>
            <span class="n">emb</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="s2">"pooler_output"</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">"last_hidden_state"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">:</span>
                <span class="k">if</span> <span class="s2">"hidden_states"</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">:</span>
                    <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="s2">"hidden_states"</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">"No last hidden state available. Use a different model or the pooler embedding method"</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="s2">"last_hidden_state"</span><span class="p">]</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="s2">"attention_mask"</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">embedding_mode</span> <span class="o">==</span> <span class="s2">"cls"</span><span class="p">:</span>
                <span class="n">emb</span> <span class="o">=</span> <span class="n">hidden_state</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">elif</span> <span class="n">embedding_mode</span> <span class="o">==</span> <span class="s2">"max"</span><span class="p">:</span>
                <span class="n">input_mask_expanded</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">hidden_state</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                <span class="n">hidden_state</span><span class="p">[</span><span class="n">input_mask_expanded</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1e9</span>  <span class="c1"># Set padding tokens to large negative value</span>
                <span class="n">emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="c1"># copied from sentence-transformers pooling</span>
            <span class="k">elif</span> <span class="n">embedding_mode</span> <span class="o">==</span> <span class="s2">"mean"</span><span class="p">:</span>
                <span class="n">input_mask_expanded</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">hidden_state</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                <span class="n">sum_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">hidden_state</span> <span class="o">*</span> <span class="n">input_mask_expanded</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">sum_mask</span> <span class="o">=</span> <span class="n">input_mask_expanded</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">emb</span> <span class="o">=</span> <span class="n">sum_embeddings</span> <span class="o">/</span> <span class="n">sum_mask</span>
            <span class="k">elif</span> <span class="n">embedding_mode</span> <span class="o">==</span> <span class="s2">"token"</span><span class="p">:</span>
                <span class="n">emb</span> <span class="o">=</span> <span class="n">hidden_state</span>
                <span class="n">task_outputs</span><span class="p">[</span><span class="s2">"word_ids"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">features</span><span class="o">.</span><span class="n">word_ids</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">input</span><span class="p">))]</span>
        <span class="n">predictions</span><span class="p">[</span><span class="s2">"embeddings"</span><span class="p">]</span> <span class="o">=</span> <span class="n">emb</span>

        <span class="k">return</span> <span class="n">PredictionOutputForEmbedding</span><span class="p">(</span><span class="n">model_outputs</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="o">**</span><span class="n">task_outputs</span><span class="p">)</span></div>

<div class="viewcode-block" id="Onnx._sequence_classification"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.onnx.Onnx._sequence_classification">[docs]</a>    <span class="k">def</span> <span class="nf">_sequence_classification</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">:</span> <span class="n">PredictionRequest</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PredictionOutput</span><span class="p">:</span>
        <span class="sd">"""</span>
<span class="sd">        Classifies the given input</span>

<span class="sd">        Args:</span>
<span class="sd">             request: The request containing e.g. the input text</span>

<span class="sd">        Returns:</span>
<span class="sd">                 The prediction output containing the predicted labels</span>
<span class="sd">        """</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
        <span class="n">task_outputs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># If logits dim &gt; 1 or if the 'is_regression' flag is not set, we assume classification:</span>
        <span class="c1"># We replace the logits by the softmax and add labels chosen with argmax</span>
        <span class="k">if</span> <span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"is_regression"</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">]</span> <span class="o">=</span> <span class="n">probabilities</span>
            <span class="n">task_outputs</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">PredictionOutputForSequenceClassification</span><span class="p">(</span><span class="n">model_outputs</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="o">**</span><span class="n">task_outputs</span><span class="p">)</span></div>

<div class="viewcode-block" id="Onnx._token_classification"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.onnx.Onnx._token_classification">[docs]</a>    <span class="k">def</span> <span class="nf">_token_classification</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">:</span> <span class="n">PredictionRequest</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PredictionOutput</span><span class="p">:</span>
        <span class="sd">"""</span>
<span class="sd">        Classifies each token of the input text</span>

<span class="sd">        Args:</span>
<span class="sd">            request: The request containing e.g. the input text</span>

<span class="sd">        Returns:</span>
<span class="sd">                 the classification output containing the labels</span>
<span class="sd">        """</span>
        <span class="n">predictions</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="n">output_features</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># If logits dim &gt; 1 or if the 'is_regression' flag is not set, we assume classification:</span>
        <span class="c1"># We replace the logits by the softmax and add labels chosen with argmax</span>
        <span class="n">task_outputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"word_ids"</span><span class="p">:</span> <span class="p">[</span><span class="n">features</span><span class="o">.</span><span class="n">word_ids</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">input</span><span class="p">))]</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"is_regression"</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">]</span> <span class="o">=</span> <span class="n">probabilities</span>
            <span class="n">task_outputs</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">PredictionOutputForTokenClassification</span><span class="p">(</span><span class="n">model_outputs</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="o">**</span><span class="n">task_outputs</span><span class="p">)</span></div>

<div class="viewcode-block" id="Onnx._generation"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.onnx.Onnx._generation">[docs]</a>    <span class="k">def</span> <span class="nf">_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">:</span> <span class="n">PredictionRequest</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PredictionOutput</span><span class="p">:</span>
        <span class="sd">"""</span>
<span class="sd">        Generates a continuation for the given input sequence</span>

<span class="sd">        Args:</span>
<span class="sd">             request: The request with e.g. the input sequence</span>

<span class="sd">        Returns:</span>
<span class="sd">             The output containing the generated text</span>
<span class="sd">        """</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="n">task_outputs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"generated_texts"</span><span class="p">:</span> <span class="p">[]}</span>
        <span class="n">model_outputs</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">request</span><span class="o">.</span><span class="n">input</span><span class="p">:</span>
            <span class="c1"># if num_beams is specified beam search is executed otherwise greedy search</span>
            <span class="k">if</span> <span class="s2">"num_beams"</span> <span class="ow">in</span> <span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="p">:</span>
                <span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beam_search</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_greedy_generation</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>

            <span class="n">generated_texts</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                     <span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                                                         <span class="s2">"clean_up_tokenization_spaces"</span><span class="p">,</span> <span class="kc">False</span><span class="p">))</span>
                               <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="p">]</span>
            <span class="n">task_outputs</span><span class="p">[</span><span class="s2">"generated_texts"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">generated_texts</span><span class="p">)</span>

            <span class="n">res</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">"sequence"</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span>
                <span class="s2">"scores"</span><span class="p">:</span> <span class="n">scores</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">model_outputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">PredictionOutputForGeneration</span><span class="p">(</span><span class="n">model_outputs</span><span class="o">=</span><span class="n">model_outputs</span><span class="p">,</span> <span class="o">**</span><span class="n">task_outputs</span><span class="p">)</span></div>

<div class="viewcode-block" id="Onnx._greedy_generation"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.onnx.Onnx._greedy_generation">[docs]</a>    <span class="k">def</span> <span class="nf">_greedy_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">max_length</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Performs greedy generation for the prompts</span>

<span class="sd">        Args:</span>
<span class="sd">             request: the inference request</span>
<span class="sd">             prompt: the prompt for the generation</span>
<span class="sd">             max_length: the maximum length of the generated sequence</span>
<span class="sd">        Returns:</span>
<span class="sd">             the ids of the generated sequence and the score</span>
<span class="sd">        """</span>
        <span class="n">cur_len</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">eos_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="p">[</span><span class="s2">"padding"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span>
                                  <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span>
                                  <span class="o">**</span><span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="p">)</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span>
        <span class="n">generated_ids</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">get_bos_token</span><span class="p">()]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="p">[]</span>
        <span class="n">unfinished_sequences</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">()</span>
        <span class="c1"># greedy generation (adapted from transformers/generation_utils.py)</span>
        <span class="k">while</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">input_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_input</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">generated_ids</span><span class="p">)</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="n">features</span><span class="o">=</span><span class="n">input_data</span><span class="p">)</span>

            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">scores</span> <span class="o">+=</span> <span class="p">(</span><span class="n">next_token_logits</span><span class="p">,)</span>

            <span class="c1"># argmax</span>
            <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># update generated ids, model inputs, and length for next step</span>
            <span class="n">generated_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">unfinished_sequences</span> <span class="o">=</span> <span class="n">unfinished_sequences</span><span class="o">.</span><span class="n">mul</span><span class="p">((</span><span class="n">next_tokens</span> <span class="o">!=</span> <span class="n">eos_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>
                <span class="c1"># stop when each sentence is finished, or if we exceed the maximum length</span>
            <span class="k">if</span> <span class="n">unfinished_sequences</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">generated_ids</span><span class="p">],</span> <span class="n">scores</span></div>

<div class="viewcode-block" id="Onnx._prepare_input"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.onnx.Onnx._prepare_input">[docs]</a>    <span class="k">def</span> <span class="nf">_prepare_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder_features</span><span class="p">,</span> <span class="n">generated_sequence</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Prepares the input for the _predict method. If the model is an encoder decoder model the</span>
<span class="sd">        generated sequence is the decoder input.</span>

<span class="sd">        Args:</span>
<span class="sd">             encoder_features: the features of the prompt</span>
<span class="sd">             generated_sequence: the generated ids</span>

<span class="sd">        Returns:</span>
<span class="sd">             the features for the model</span>
<span class="sd">        """</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">encoder_features</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="c1"># if it is a encoder decoder model the generated sequence is passed to the decoder</span>
        <span class="c1"># otherwise the generated sequence is appended to the input_ids</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="n">model_input</span><span class="p">[</span><span class="s2">"decoder_input_ids"</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">generated_sequence</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
            <span class="n">model_input</span><span class="p">[</span><span class="s2">"decoder_attention_mask"</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">generated_sequence</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">generated_sequence</span><span class="p">:</span>
            <span class="n">model_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">(</span><span class="n">encoder_features</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">generated_sequence</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">model_input</span><span class="p">[</span><span class="s2">"attention_mask"</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">model_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">model_input</span></div>

<div class="viewcode-block" id="Onnx._beam_search"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.onnx.Onnx._beam_search">[docs]</a>    <span class="k">def</span> <span class="nf">_beam_search</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">max_length</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Performs beam search for the given prompt</span>

<span class="sd">        Args:</span>
<span class="sd">             request: the inference request</span>
<span class="sd">             prompt: the generation prompt</span>
<span class="sd">             max_length: the maximum length of the generated sequence</span>

<span class="sd">        Returns:</span>
<span class="sd">             the generated sequence(s)</span>
<span class="sd">        """</span>
        <span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="p">[</span><span class="s2">"padding"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span>
                                  <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span>
                                  <span class="o">**</span><span class="n">request</span><span class="o">.</span><span class="n">preprocessing_kwargs</span><span class="p">)</span>
        <span class="c1"># get the generation arguments</span>
        <span class="n">num_beams</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"num_beams"</span><span class="p">)</span>
        <span class="n">no_repeat_ngram_size</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"no_repeat_ngram_size"</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">do_sample</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"do_sample"</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">top_p</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"top_p"</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">top_k</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"top_k"</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">task_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"num_return_sequences"</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">return_sequences</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_return_sequences</span><span class="p">):</span>
            <span class="n">sequences</span> <span class="o">=</span> <span class="p">[([</span><span class="bp">self</span><span class="o">.</span><span class="n">get_bos_token</span><span class="p">()]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="p">[],</span> <span class="mf">0.0</span><span class="p">)]</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">while</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">max_length</span><span class="p">:</span>
                <span class="n">candidates</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">sequences</span><span class="p">:</span>
                    <span class="n">model_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_input</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">seq</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                    <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">model_input</span><span class="p">)</span>

                    <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="s2">"logits"</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
                    <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>
                        <span class="n">next_token_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_preprocess_logits</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
                                                                    <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">num_beams</span><span class="p">)</span>

                    <span class="k">if</span> <span class="n">no_repeat_ngram_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">banned_batch_tokens</span> <span class="o">=</span> <span class="n">calc_banned_ngram_tokens</span><span class="p">(</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">seq</span><span class="p">[</span><span class="mi">0</span><span class="p">]]),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="p">,</span> <span class="n">cur_len</span>
                        <span class="p">)</span>
                        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">banned_tokens</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">banned_batch_tokens</span><span class="p">):</span>
                            <span class="n">next_token_scores</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">banned_tokens</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">"inf"</span><span class="p">)</span>

                    <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>
                        <span class="c1"># draw the next token based on the probabilities</span>
                        <span class="n">probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

                        <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">)</span>
                        <span class="n">next_token_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">)</span>

                        <span class="n">_</span><span class="p">,</span> <span class="n">_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">next_token_prob</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                        <span class="n">next_tokens_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">_indices</span><span class="p">)</span>

                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># take the most likely tokens as the next tokens</span>
                        <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_tokens_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">candidates</span> <span class="o">+=</span> <span class="p">[(</span><span class="n">seq</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">token_id</span><span class="p">],</span> <span class="n">seq</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">score</span><span class="p">))</span> <span class="k">for</span> <span class="n">token_id</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span>
                                   <span class="nb">zip</span><span class="p">(</span><span class="n">next_tokens_idx</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">next_token_prob</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())]</span>
                <span class="c1"># select the candidates with the highest scores as beams for the generation of the next token</span>
                <span class="n">sequences</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">candidates</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span><span class="n">num_beams</span><span class="p">]</span>
                <span class="n">cur_len</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="n">return_sequences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sequences</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="k">return</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">return_sequences</span><span class="p">],</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">return_sequences</span><span class="p">]</span></div>

<div class="viewcode-block" id="Onnx.get_bos_token"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.onnx.Onnx.get_bos_token">[docs]</a>    <span class="k">def</span> <span class="nf">get_bos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Depending on the model the beginning of sentence token id can be different or not provided.</span>

<span class="sd">        Returns:</span>
<span class="sd">             beginning of sentence token id</span>
<span class="sd">        """</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token_id</span>
        <span class="c1"># if no bos token is available use the padding token</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span></div>

<div class="viewcode-block" id="Onnx._preprocess_logits"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.onnx.Onnx._preprocess_logits">[docs]</a>    <span class="k">def</span> <span class="nf">_preprocess_logits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">filter_value</span><span class="o">=-</span><span class="nb">float</span><span class="p">(</span><span class="s2">"Inf"</span><span class="p">)):</span>
        <span class="sd">"""</span>
<span class="sd">        Sets the scores for all tokens that are not in top_p and top_k to the filter value (default = -inf).</span>
<span class="sd">        Adapted from huggingface/transformers/generation_logits_process.py</span>

<span class="sd">        Args:</span>
<span class="sd">             scores:the initial scores for each token</span>
<span class="sd">             top_k: the top_k threshold</span>
<span class="sd">             top_p: the top_p threshold</span>
<span class="sd">             min_tokens_to_keep: the min_number_of_tokens_to_keep</span>
<span class="sd">             filter_value: the value to replace the scores with</span>

<span class="sd">        Returns:</span>
<span class="sd">             the processed scores</span>
<span class="sd">        """</span>
        <span class="k">if</span> <span class="n">top_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">top_k</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">top_k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="p">),</span> <span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># Safety check</span>
            <span class="c1"># Remove all tokens with a probability less than the last token of the top-k</span>
            <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">indices_to_remove</span><span class="p">,</span> <span class="n">filter_value</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">top_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">top_p</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="n">sorted_logits</span><span class="p">,</span> <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">cumulative_probs</span> <span class="o">=</span> <span class="n">sorted_logits</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Remove tokens with cumulative top_p above the threshold (token with 0 are kept)</span>
            <span class="n">sorted_indices_to_remove</span> <span class="o">=</span> <span class="n">cumulative_probs</span> <span class="o">&gt;</span> <span class="n">top_p</span>
            <span class="k">if</span> <span class="n">min_tokens_to_keep</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)</span>
                <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="n">min_tokens_to_keep</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># Shift the indices to the right to keep also the first token above the threshold</span>
            <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># scatter sorted tensors to original indexing</span>
            <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">sorted_indices_to_remove</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="p">,</span> <span class="n">sorted_indices_to_remove</span><span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">indices_to_remove</span><span class="p">,</span> <span class="n">filter_value</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">scores</span></div></div>


<div class="viewcode-block" id="calc_banned_ngram_tokens"><a class="viewcode-back" href="../../../pages/api/model_api/inference.html#square_model_inference.inference.onnx.calc_banned_ngram_tokens">[docs]</a><span class="k">def</span> <span class="nf">calc_banned_ngram_tokens</span><span class="p">(</span><span class="n">prev_input_ids</span><span class="p">,</span> <span class="n">num_hypos</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">"""Copied from fairseq for no_repeat_ngram in beam_search"""</span>
    <span class="k">if</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="n">no_repeat_ngram_size</span><span class="p">:</span>
        <span class="c1"># return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet</span>
        <span class="k">return</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hypos</span><span class="p">)]</span>
    <span class="n">generated_ngrams</span> <span class="o">=</span> <span class="p">[{}</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hypos</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hypos</span><span class="p">):</span>
        <span class="n">gen_tokens</span> <span class="o">=</span> <span class="n">prev_input_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="n">generated_ngram</span> <span class="o">=</span> <span class="n">generated_ngrams</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">ngram</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">gen_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">no_repeat_ngram_size</span><span class="p">)]):</span>
            <span class="n">prev_ngram_tuple</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">ngram</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">generated_ngram</span><span class="p">[</span><span class="n">prev_ngram_tuple</span><span class="p">]</span> <span class="o">=</span> <span class="n">generated_ngram</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">prev_ngram_tuple</span><span class="p">,</span> <span class="p">[])</span> <span class="o">+</span> <span class="p">[</span><span class="n">ngram</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>

    <span class="k">def</span> <span class="nf">_get_generated_ngrams</span><span class="p">(</span><span class="n">hypo_idx</span><span class="p">):</span>
        <span class="c1"># Before decoding the next token, prevent decoding of ngrams that have already appeared</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">no_repeat_ngram_size</span>
        <span class="n">ngram_idx</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">prev_input_ids</span><span class="p">[</span><span class="n">hypo_idx</span><span class="p">,</span> <span class="n">start_idx</span><span class="p">:</span><span class="n">cur_len</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">generated_ngrams</span><span class="p">[</span><span class="n">hypo_idx</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">ngram_idx</span><span class="p">,</span> <span class="p">[])</span>

    <span class="n">banned_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">_get_generated_ngrams</span><span class="p">(</span><span class="n">hypo_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">hypo_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hypos</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">banned_tokens</span></div>
</pre>
       </div>
      </article>
     </div>
    </div>
   </main>
  </div>
  <footer class="md-footer">
   <div class="md-footer-nav">
    <nav class="md-footer-nav__inner md-grid">
    </nav>
   </div>
   <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
     <div class="md-footer-copyright">
      <div class="md-footer-copyright__highlight">
       © Copyright 2021, UKP.
      </div>
      Created using
      <a href="http://www.sphinx-doc.org/">
       Sphinx
      </a>
      4.3.2.
             and
      <a href="https://github.com/bashtage/sphinx-material/">
       Material for
              Sphinx
      </a>
     </div>
    </div>
   </div>
  </footer>
  <script src="../../../_static/javascripts/application.js">
  </script>
  <script>
   app.initialize({version: "1.0.4", url: {base: ".."}})
  </script>
 </body>
</html>