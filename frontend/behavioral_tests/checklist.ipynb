{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on how to run Checklist (aka Behavioral Tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add functionality to process json to be injected into the db\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checklist is a list of predefined questions that evaluate the behavior of a model (SQuARE Skill). In particular, it evalutes whether the Skill has certain abilities such as understanding comparisons, understanding coreference, or even it let us know if the model has gender biases (eg: he is a doctor, she is a nurse).\n",
    "\n",
    "\n",
    "To do this we need the following 2 methods: `create_query` and `predict`. For simplicity I copied them below but you can find them on checklist.py in your current folder.\n",
    "\n",
    "We need first the Skill we want to evaluate and also the list of test_cases. You can find all test cases in the folder explainability-api/checklists. We have test cases for extractive skills, for multiple-choice and for boolean skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query(skill, test_cases: List):\n",
    "    \"\"\"\n",
    "    Creates a query and make it suitable for sending to for prediction\n",
    "\n",
    "    Args:\n",
    "        skill: input skill for which the checklist tests are run\n",
    "        test_cases (list) : Test cases as a list\n",
    "\n",
    "    Returns:\n",
    "        json_object (json object) : A json object containing the test case and its prediction\n",
    "        answer (str) : Prediction for test case made by the skill\n",
    "\n",
    "    \"\"\"\n",
    "    skill_type = skill[\"skill_type\"]\n",
    "    base_model = skill[\"default_skill_args\"].get(\"base_model\")\n",
    "    adapter = skill[\"default_skill_args\"].get(\"adapter\")\n",
    "    # extract all tests\n",
    "    all_tests = [tests[\"test_cases\"] for tests in test_cases]\n",
    "    # all_tests = list(itertools.chain.from_iterable([tests[\"test_cases\"] for tests in test_cases]))\n",
    "    questions, contexts, answers = list(), list(), list()\n",
    "\n",
    "    test_type = list(itertools.chain.from_iterable([[test[\"test_type\"]] * len(test[\"test_cases\"])\n",
    "                                                    for test in test_cases]))\n",
    "    capability = list(itertools.chain.from_iterable([[test[\"capability\"]] * len(test[\"test_cases\"])\n",
    "                                                    for test in test_cases]))\n",
    "    test_name = list(itertools.chain.from_iterable([[test[\"test_name\"]] * len(test[\"test_cases\"])\n",
    "                                                    for test in test_cases]))\n",
    "\n",
    "    for tests in all_tests:\n",
    "        questions.append([query[\"question\"] for query in tests])\n",
    "        # list of list for mcq else list\n",
    "        contexts.append([query[\"context\"] if skill_type != \"multiple-choice\"\n",
    "                         else query[\"context\"] + \"\\n\" + \"\\n\".join(query[\"options\"])\n",
    "                         for query in tests])\n",
    "        answers.extend([query.get(\"answer\") if \"answer\" in query.keys() else query.get(\"prediction_before_change\")\n",
    "                        for query in tests])\n",
    "\n",
    "        # TODO\n",
    "        # send batch to the skill query endpoint\n",
    "\n",
    "    prediction_requests = list()\n",
    "    # create the prediction request\n",
    "    for idx in range(len(questions)):\n",
    "        for question, context in zip(questions[idx], contexts[idx]):\n",
    "            request = dict()\n",
    "            request[\"num_results\"] = 1\n",
    "            request[\"user_id\"] = \"ukp\"\n",
    "            request[\"skill_args\"] = {\"base_model\": base_model, \"adapter\": adapter, \"context\": context}\n",
    "            request[\"query\"] = question\n",
    "            prediction_requests.append(request)\n",
    "\n",
    "    model_inputs = dict()\n",
    "    model_inputs[\"request\"] = prediction_requests\n",
    "    model_inputs[\"answers\"] = answers\n",
    "    model_inputs[\"test_type\"] = test_type\n",
    "    model_inputs[\"capability\"] = capability\n",
    "    model_inputs[\"test_name\"] = test_name\n",
    "    # logger.info(\"inputs:\", model_inputs)\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def predict(model_inputs: dict, skill_id: str) -> list:\n",
    "    \"\"\"\n",
    "    Predicts a given query\n",
    "\n",
    "    Args:\n",
    "        model_inputs (dict) : input for the model inference\n",
    "        skill_id (str) : id of skill for which the predictions need to be run\n",
    "\n",
    "    Returns:\n",
    "        Returns the model predictions and success rate\n",
    "    \"\"\"\n",
    "    model_outputs = list()\n",
    "    try:\n",
    "        headers = {'Content-type': 'application/json'}\n",
    "        skill_query_url = f\"https://square.ukp-lab.de/api/skill-manager/skill/{skill_id}/query\" #note I hardcoded square URL here\n",
    "        model_predictions = list()\n",
    "        # i = 0\n",
    "        for request in tqdm(model_inputs[\"request\"]):\n",
    "            request['preprocessing_kwargs'] = {\"max_length\": 512}\n",
    "            response = requests.post(skill_query_url, data=json.dumps(request), headers=headers)\n",
    "            predictions = response.json()\n",
    "            model_predictions.append(predictions[\"predictions\"][0][\"prediction_output\"][\"output\"])\n",
    "            # i += 1\n",
    "            # if i == 10:\n",
    "            #     break\n",
    "\n",
    "        # calculate success rate\n",
    "        success_rate = [pred == gold for pred, gold in zip(model_predictions, model_inputs[\"answers\"])]\n",
    "\n",
    "        for test_type, capability, test_name, request, answer, prediction, success in zip(\n",
    "            model_inputs[\"test_type\"],\n",
    "            model_inputs[\"capability\"],\n",
    "            model_inputs[\"test_name\"],\n",
    "            model_inputs[\"request\"],\n",
    "            model_inputs[\"answers\"],\n",
    "            model_predictions,\n",
    "            success_rate\n",
    "        ):\n",
    "            model_outputs.append(\n",
    "                {\n",
    "                    \"skill_id\": skill_id,\n",
    "                    \"test_type\": test_type,\n",
    "                    \"capability\": capability,\n",
    "                    \"test_name\": test_name,\n",
    "                    \"question\": request[\"query\"],\n",
    "                    \"context\": request[\"skill_args\"][\"context\"],\n",
    "                    \"answer\": answer,\n",
    "                    \"prediction\": prediction,\n",
    "                    \"success\": success\n",
    "                }\n",
    "            )\n",
    "        # print(model_outputs)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    return model_outputs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get SQuARE's Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_response = requests.get(\"https://square.ukp-lab.de/api/skill-manager/skill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = skills_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '64146bf68aab0a390550e4bb',\n",
       " 'name': 'Roberta adapter hellaswag',\n",
       " 'url': 'http://multiple-choice-qa',\n",
       " 'skill_type': 'multiple-choice',\n",
       " 'skill_settings': {'requires_context': False, 'requires_multiple_choices': 0},\n",
       " 'user_id': 'puerto',\n",
       " 'created_at': '2023-03-17T13:32:38.563000',\n",
       " 'skill_input_examples': [{'query': '', 'context': '', 'choices': ['', '']},\n",
       "  {'query': '', 'context': '', 'choices': ['', '']},\n",
       "  {'query': '', 'context': '', 'choices': ['', '']}],\n",
       " 'models': {'reader': 'roberta-base'},\n",
       " 'description': '',\n",
       " 'default_skill_args': {'base_model': 'roberta-base',\n",
       "  'average_adapters': False,\n",
       "  'adapter': 'AdapterHub/roberta-base-pf-hellaswag'},\n",
       " 'published': True,\n",
       " 'meta_skill': False,\n",
       " 'client_id': 'puerto-Roberta adapter hellaswag',\n",
       " 'client_secret': None,\n",
       " 'data_sets': ['hellaswag']}"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 SpanBert - SQuAD 63a61a162e30fd4c06f7a0b6 haritzpuerto/spanbert-large-cased_SQuAD\n",
      "1 SpanBert - DuoRC 63a61c802e30fd4c06f7a0b7 haritzpuerto/spanbert-large-cased_DuoRC\n",
      "2 SpanBert - HotpotQA 63a61cc32e30fd4c06f7a0b8 haritzpuerto/spanbert-large-cased_HotpotQA\n",
      "3 SpanBert - NewsQA 63a61ce70bf4c04a19c59dc8 haritzpuerto/spanbert-large-cased_NewsQA\n",
      "4 SpanBert - QAMR 63a61d300bf4c04a19c59dc9 haritzpuerto/spanbert-large-cased_QAMR\n",
      "5 SpanBert - SearchQA 63a61d7d2e30fd4c06f7a0b9 haritzpuerto/spanbert-large-cased_SearchQA\n",
      "6 SpanBert - TriviaQA-web 63a61db22e30fd4c06f7a0ba haritzpuerto/spanbert-large-cased_TriviaQA-web\n",
      "7 SpanBert - NaturalQuestionsShort 63a6246c2e30fd4c06f7a0be haritzpuerto/spanbert-large-cased_NaturalQuestionsShort\n",
      "8 SpanBert - HotpotQA - Onnx 63f7aacb27118129ac5d62b4 UKP-SQuARE/spanbert-large-cased_HotpotQA-onnx\n"
     ]
    }
   ],
   "source": [
    "spanbertskills = [skill for skill in skills if \"spanbert\" in skill[\"name\"].lower()]\n",
    "count = 0\n",
    "for skill in spanbertskills:\n",
    "    print(count, skill[\"name\"], skill[\"id\"], skill['models']['reader'])\n",
    "    count += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../checklists/extractive_model_tests.json\") as f:\n",
    "    extractive_model_tests = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's make the query\n",
    "\n",
    "The query is actually a list of queries to the Skill. For each test, we gotta make a query to the Skill to get the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last model is 7, don't do onnx models\n",
    "current_skill = 7\n",
    "skill = spanbertskills[current_skill]\n",
    "query = create_query(skill, extractive_model_tests['tests'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['request', 'answers', 'test_type', 'capability', 'test_name'])"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of getting some result quickly, I will just use 1 test, but you should try with all tests too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_min = query.copy()\n",
    "# query_min['request'] = query_min['request'][317:318]\n",
    "# query_min['answers'] = query_min['answers'][317:318]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get the predictions of the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 692/692 [25:54<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "model_outputs = predict(query, skill['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model_outputs to json file\n",
    "file_name = skill['name'] + \"_\" + skill['id']\n",
    "with open(f\"{file_name}.json\", \"x\") as f:\n",
    "    json.dump(model_outputs, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the `predicition` is different from the `answer` (label). This shows that the model is not understanding comparisons properly. If this happens in most comparisons tests, this will indicate that the model is weak when doing comparisons, and shouldn't be used for that case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should run at least one extractive QA Skill with the tests (checklist). I would recommend the one of the example above or any with the name `SpanBert`-{DATASET} because these Skills should be high performing, so it would be interesting to know if they are always robust or if there is any kind of reasoning where it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checklist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f4b1df9402f5c9ae5cac5f40ef98bbd391cfcb59c4c0c638fee2fda7003ede2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
